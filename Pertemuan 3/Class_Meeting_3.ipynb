{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhajirakbarhsb/NLP_class_2023/blob/main/Class_Meeting_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE_kvc-qSpmU"
      },
      "source": [
        "# Text Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDjT9IiHXTdf"
      },
      "source": [
        "## OneHot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40DvncoZXSpO",
        "outputId": "e7335bb8-bc14-4a23-89ff-9c352583a725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: cat\n",
            "One-Hot Encoded Vector: [0. 0. 1. 0.]\n",
            "Decoded Text: cat\n",
            "\n",
            "Original Text: dog\n",
            "One-Hot Encoded Vector: [0. 0. 0. 1.]\n",
            "Decoded Text: dog\n",
            "\n",
            "Original Text: bat\n",
            "One-Hot Encoded Vector: [0. 1. 0. 0.]\n",
            "Decoded Text: bat\n",
            "\n",
            "Original Text: ate\n",
            "One-Hot Encoded Vector: [1. 0. 0. 0.]\n",
            "Decoded Text: ate\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import itertools\n",
        "\n",
        "# two example documents\n",
        "docs = [\"cat\", \"dog\", \"bat\", \"ate\"]\n",
        "\n",
        "# Split documents into tokens\n",
        "tokens_docs = [doc.split(\" \") for doc in docs]\n",
        "\n",
        "# Convert list of token-lists to one flat list of tokens\n",
        "# and then create a dictionary that maps word to id of word\n",
        "all_tokens = itertools.chain.from_iterable(tokens_docs)\n",
        "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
        "\n",
        "# Convert token lists to token-id lists\n",
        "token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
        "\n",
        "# Convert list of token-id lists to one-hot representation\n",
        "vec = OneHotEncoder(categories=\"auto\")\n",
        "X = vec.fit_transform(token_ids)\n",
        "\n",
        "# Convert the one-hot encoded vectors back to text representations\n",
        "inverse_mapping = {idx: token for token, idx in word_to_id.items()}\n",
        "\n",
        "# Display the original text representations\n",
        "for i, row in enumerate(X.toarray()):\n",
        "    print(f\"Original Text: {docs[i]}\")\n",
        "    print(\"One-Hot Encoded Vector:\", row)\n",
        "    decoded_text = [inverse_mapping[idx] for idx, val in enumerate(row) if val == 1]\n",
        "    print(\"Decoded Text:\", \" \".join(decoded_text))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-7xj9STXx8G",
        "outputId": "2a93241d-0a4b-4ac5-b0dc-881c70f39cfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['cat'], ['dog'], ['bat'], ['ate']]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnwvLIEgX4gJ",
        "outputId": "1f1ebc48-178b-47df-f830-2a07ea8cbec7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ate': 0, 'bat': 1, 'cat': 2, 'dog': 3}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_to_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBpYJpWQalTh"
      },
      "source": [
        "## Bag of Words countVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKgHzKzKaraU",
        "outputId": "2377cce5-5274-4b6e-fb34-539ff3a49968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'love': 2, 'nlp': 3, 'is': 1, 'so': 4, 'cool': 0}\n",
            "(1, 5)\n",
            "[[1 1 1 2 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = [\"i love nlp. nlp is so cool\"]\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.vocabulary_)\n",
        "# Output: {'love': 2, 'nlp': 3, 'is': 1, 'so': 4, 'cool': 0}\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape) # Output: (1, 5)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYu9jQZlVQ0"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MRmPzn7lUCZ",
        "outputId": "76506fb9-b49e-41dc-da47-95e69b55cfff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Alharidt\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Alharidt\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Alharidt\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['news mentioned fake', 'audience encourage fake news', 'fake news false misleading']\n",
            "[['news', 'mentioned', 'fake'], ['audience', 'encourage', 'fake', 'news'], ['fake', 'news', 'false', 'misleading']]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "paragraph = \"\"\"The news mentioned here is fake. Audience do not encourage fake news. Fake news is false or misleading\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "corpus = []\n",
        "\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    sent = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    sent = sent.lower()\n",
        "    sent= sent.split()\n",
        "    sent = [lemmatizer.lemmatize(word) for word in sent if not word in set(stopwords.words('english'))]\n",
        "    sent = ' '.join(sent)\n",
        "    corpus.append(sent)\n",
        "\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "\n",
        "words_unique = []\n",
        "for i in range(len(corpus)):\n",
        "    unique = nltk.word_tokenize(corpus[i])\n",
        "    words_unique.append(unique)\n",
        "\n",
        "print(words_unique)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnIt8-LylrNW",
        "outputId": "73870c7d-6351-446d-bca8-341df78115ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            audience  encourage      fake     false  mentioned  misleading  \\\n",
            "Document 1  0.000000   0.000000  0.453295  0.000000   0.767495    0.000000   \n",
            "Document 2  0.608845   0.608845  0.359594  0.000000   0.000000    0.000000   \n",
            "Document 3  0.000000   0.000000  0.359594  0.608845   0.000000    0.608845   \n",
            "\n",
            "                news  \n",
            "Document 1  0.453295  \n",
            "Document 2  0.359594  \n",
            "Document 3  0.359594  \n"
          ]
        }
      ],
      "source": [
        "# Creating the TF-IDF model\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "independentFeatures_tfIDF = tfidf.fit_transform(corpus).toarray()\n",
        "tfidf_df = pd.DataFrame(data=independentFeatures_tfIDF, columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Add an index column to represent the documents\n",
        "tfidf_df.index = [f\"Document {i+1}\" for i in range(len(corpus))]\n",
        "\n",
        "# Display the TF-IDF DataFrame\n",
        "print(tfidf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyMD7kijybzN"
      },
      "source": [
        "## n-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zCZC_kfyfAl",
        "outputId": "6b57bb36-b6c3-49e4-ab4c-d7b1d19038c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-grams:\n",
            "('This', 'is')\n",
            "('is', 'an')\n",
            "('an', 'example')\n",
            "('example', 'sentence')\n",
            "('sentence', 'for')\n",
            "('for', 'generating')\n",
            "('generating', 'n')\n",
            "('n', 'grams')\n",
            "\n",
            "Tri-grams:\n",
            "('This', 'is', 'an')\n",
            "('is', 'an', 'example')\n",
            "('an', 'example', 'sentence')\n",
            "('example', 'sentence', 'for')\n",
            "('sentence', 'for', 'generating')\n",
            "('for', 'generating', 'n')\n",
            "('generating', 'n', 'grams')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    # Tokenize the text into words\n",
        "    words = re.findall(r'\\w+', text)\n",
        "\n",
        "    # Generate n-grams using NLTK's ngrams function\n",
        "    ngrams_list = list(ngrams(words, n))\n",
        "\n",
        "    return ngrams_list\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence for generating n-grams.\"\n",
        "\n",
        "# Generate bi-grams (2-grams)\n",
        "bigrams = generate_ngrams(text, 2)\n",
        "print(\"Bi-grams:\")\n",
        "for gram in bigrams:\n",
        "    print(gram)\n",
        "\n",
        "# Generate tri-grams (3-grams)\n",
        "trigrams = generate_ngrams(text, 3)\n",
        "print(\"\\nTri-grams:\")\n",
        "for gram in trigrams:\n",
        "    print(gram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YvO9A-pSy4G"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zYdJmXjFNKZw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "TPp6i1UAS-gn",
        "outputId": "798eafc0-3f3f-478d-e568-77efad95422a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>at</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>userName</th>\n",
              "      <th>contentp</th>\n",
              "      <th>contentp_clean</th>\n",
              "      <th>text_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-11-19 08:08:17</td>\n",
              "      <td>Paketnya mahal2 dan data internetnya juga boro...</td>\n",
              "      <td>1</td>\n",
              "      <td>Wildan Cell</td>\n",
              "      <td>paketnya mahal dan data internetnya juga boros...</td>\n",
              "      <td>paket mahal data internetnya boros pokok rekom...</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-11-19 00:02:23</td>\n",
              "      <td>Ulasan lama : Parah ini kenapa ga bisa login d...</td>\n",
              "      <td>4</td>\n",
              "      <td>Deyubi kyubi</td>\n",
              "      <td>ulasan lama parah ini kenapa ga bisa login di ...</td>\n",
              "      <td>ulas lama parah kenapa bisa login aplikasi upd...</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-11-18 22:02:21</td>\n",
              "      <td>Susah banget mau login ke aplikasi sampe jengkel</td>\n",
              "      <td>1</td>\n",
              "      <td>Asyraf Kaylani</td>\n",
              "      <td>susah banget mau login ke aplikasi sampe jengkel</td>\n",
              "      <td>susah banget mau login aplikasi jengkel</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-11-18 18:21:34</td>\n",
              "      <td>Cukup bagus tapi sayang tidak ada menu blokir ...</td>\n",
              "      <td>5</td>\n",
              "      <td>Ardi Ardian</td>\n",
              "      <td>cukup bagus tapi sayang tidak ada menu blokir ...</td>\n",
              "      <td>cukup bagus sayang ada menu blokir buat lain m...</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-11-18 17:02:54</td>\n",
              "      <td>Kencang dan stabil tapi mahaaallll,.. dimurahi...</td>\n",
              "      <td>3</td>\n",
              "      <td>Bagas Eksanudin Aziz</td>\n",
              "      <td>kencang dan stabil tapi mahaaallll dimurahin d...</td>\n",
              "      <td>kencang stabil mahaaallll dimurahin sedikit ka...</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2023-11-18 12:13:53</td>\n",
              "      <td>Sekarang sering ilang ilangan sinyal e</td>\n",
              "      <td>4</td>\n",
              "      <td>Ajeng Tyas</td>\n",
              "      <td>sekarang sering ilang ilangan sinyal</td>\n",
              "      <td>sekarang sering ilang ilangan sinyal</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-11-18 10:56:10</td>\n",
              "      <td>waspada sama orbit, aktifin paket. sampai 2 ha...</td>\n",
              "      <td>1</td>\n",
              "      <td>Sahwana Shugra</td>\n",
              "      <td>waspada sama orbit aktifin paket sampai hari b...</td>\n",
              "      <td>waspada sama orbit aktifin paket hari aktif la...</td>\n",
              "      <td>222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023-11-18 10:36:19</td>\n",
              "      <td>Akun dan imei orbit tidak terdaftar sendiri pa...</td>\n",
              "      <td>1</td>\n",
              "      <td>Ilhan Mansiz</td>\n",
              "      <td>akun dan imei orbit tidak terdaftar sendiri pa...</td>\n",
              "      <td>akun imei orbit daftar sendiri padahal langgan...</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-11-18 07:39:59</td>\n",
              "      <td>😲😮😧 larang² paketane om 🤦</td>\n",
              "      <td>2</td>\n",
              "      <td>Putro Ragil</td>\n",
              "      <td>larang² paketane om</td>\n",
              "      <td>larang paketane om</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023-11-17 19:51:10</td>\n",
              "      <td>Produk gagal, jatingan tidak stabil.</td>\n",
              "      <td>1</td>\n",
              "      <td>Jonny Arung</td>\n",
              "      <td>produk gagal jatingan tidak stabil</td>\n",
              "      <td>produk gagal jatingan stabil</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    at                                            content  \\\n",
              "0  2023-11-19 08:08:17  Paketnya mahal2 dan data internetnya juga boro...   \n",
              "1  2023-11-19 00:02:23  Ulasan lama : Parah ini kenapa ga bisa login d...   \n",
              "2  2023-11-18 22:02:21   Susah banget mau login ke aplikasi sampe jengkel   \n",
              "3  2023-11-18 18:21:34  Cukup bagus tapi sayang tidak ada menu blokir ...   \n",
              "4  2023-11-18 17:02:54  Kencang dan stabil tapi mahaaallll,.. dimurahi...   \n",
              "5  2023-11-18 12:13:53             Sekarang sering ilang ilangan sinyal e   \n",
              "6  2023-11-18 10:56:10  waspada sama orbit, aktifin paket. sampai 2 ha...   \n",
              "7  2023-11-18 10:36:19  Akun dan imei orbit tidak terdaftar sendiri pa...   \n",
              "8  2023-11-18 07:39:59                          😲😮😧 larang² paketane om 🤦   \n",
              "9  2023-11-17 19:51:10               Produk gagal, jatingan tidak stabil.   \n",
              "\n",
              "   score              userName  \\\n",
              "0      1           Wildan Cell   \n",
              "1      4          Deyubi kyubi   \n",
              "2      1        Asyraf Kaylani   \n",
              "3      5           Ardi Ardian   \n",
              "4      3  Bagas Eksanudin Aziz   \n",
              "5      4            Ajeng Tyas   \n",
              "6      1        Sahwana Shugra   \n",
              "7      1          Ilhan Mansiz   \n",
              "8      2           Putro Ragil   \n",
              "9      1           Jonny Arung   \n",
              "\n",
              "                                            contentp  \\\n",
              "0  paketnya mahal dan data internetnya juga boros...   \n",
              "1  ulasan lama parah ini kenapa ga bisa login di ...   \n",
              "2   susah banget mau login ke aplikasi sampe jengkel   \n",
              "3  cukup bagus tapi sayang tidak ada menu blokir ...   \n",
              "4  kencang dan stabil tapi mahaaallll dimurahin d...   \n",
              "5               sekarang sering ilang ilangan sinyal   \n",
              "6  waspada sama orbit aktifin paket sampai hari b...   \n",
              "7  akun dan imei orbit tidak terdaftar sendiri pa...   \n",
              "8                                larang² paketane om   \n",
              "9                 produk gagal jatingan tidak stabil   \n",
              "\n",
              "                                      contentp_clean  text_length  \n",
              "0  paket mahal data internetnya boros pokok rekom...           72  \n",
              "1  ulas lama parah kenapa bisa login aplikasi upd...           73  \n",
              "2            susah banget mau login aplikasi jengkel           39  \n",
              "3  cukup bagus sayang ada menu blokir buat lain m...           60  \n",
              "4  kencang stabil mahaaallll dimurahin sedikit ka...           70  \n",
              "5               sekarang sering ilang ilangan sinyal           36  \n",
              "6  waspada sama orbit aktifin paket hari aktif la...          222  \n",
              "7  akun imei orbit daftar sendiri padahal langgan...           59  \n",
              "8                                 larang paketane om           18  \n",
              "9                       produk gagal jatingan stabil           28  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('clean_review_all.csv')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LFsmQALlyw8b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def text_representation(df, text_column, method='one-hot', ngram_range=(1, 1)):\n",
        "    \"\"\"\n",
        "    Apply various text representation techniques to a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with 'date' and 'text' columns.\n",
        "    - text_column: Name of the text column.\n",
        "    - method: Text representation method ('one-hot', 'bag-of-words', 'ngram', 'countvectorize', 'tfidf').\n",
        "    - ngram_range: Tuple specifying the n-gram range (e.g., (1, 1) for unigrams, (1, 2) for unigrams and bigrams).\n",
        "\n",
        "    Returns:\n",
        "    - Transformed DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    if method == 'one-hot':\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        words = [text.split() for text in df[text_column]]\n",
        "        one_hot_encoded = mlb.fit_transform(words)\n",
        "        one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
        "        return pd.concat([df, one_hot_df], axis=1)\n",
        "\n",
        "    elif method == 'bag-of-words':\n",
        "        vectorizer = CountVectorizer()\n",
        "        bow = vectorizer.fit_transform(df[text_column])\n",
        "        bow_df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        return pd.concat([df, bow_df], axis=1)\n",
        "\n",
        "    elif method == 'ngram':\n",
        "        vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
        "        ngram_features = vectorizer.fit_transform(df[text_column])\n",
        "        ngram_df = pd.DataFrame(ngram_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        return pd.concat([df, ngram_df], axis=1)\n",
        "\n",
        "    elif method == 'countvectorize':\n",
        "        vectorizer = CountVectorizer()\n",
        "        count_vectorized = vectorizer.fit_transform(df[text_column])\n",
        "        countvectorize_df = pd.DataFrame(count_vectorized.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        return pd.concat([df, countvectorize_df], axis=1)\n",
        "\n",
        "    elif method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf = vectorizer.fit_transform(df[text_column])\n",
        "        tfidf_df = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        return pd.concat([df, tfidf_df], axis=1)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid method. Supported methods are 'one-hot', 'bag-of-words', 'ngram', 'countvectorize', and 'tfidf'.\")\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a DataFrame called 'df' with 'date' and 'text' columns\n",
        "# transformed_df = text_representation(df, 'text', method='one-hot')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZyQADgqGTE-H"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "np.nan is an invalid document, expected byte or unicode string.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Alharidt\\Documents\\GitHub\\text-Analysis\\Pertemuan 3\\Class_Meeting_3.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Assuming you have a DataFrame called 'df' with 'date' and 'text' columns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m transformed_df \u001b[39m=\u001b[39m text_representation(df, \u001b[39m'\u001b[39;49m\u001b[39mcontentp_clean\u001b[39;49m\u001b[39m'\u001b[39;49m, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtfidf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "\u001b[1;32mc:\\Users\\Alharidt\\Documents\\GitHub\\text-Analysis\\Pertemuan 3\\Class_Meeting_3.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     tfidf \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(df[text_column])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     tfidf_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tfidf\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mconcat([df, tfidf_df], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Alharidt\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Alharidt\\miniconda3\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Alharidt\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Alharidt\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[1;32mc:\\Users\\Alharidt\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
            "File \u001b[1;32mc:\\Users\\Alharidt\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    235\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[1;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
            "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "# Assuming you have a DataFrame called 'df' with 'date' and 'text' columns\n",
        "transformed_df = text_representation(df, 'contentp_clean', method='tfidf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MjWK9QHxzCAm",
        "outputId": "207ba5e2-3fff-4ace-cd3b-ab08126745b9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'transformed_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Alharidt\\Documents\\GitHub\\text-Analysis\\Pertemuan 3\\Class_Meeting_3.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alharidt/Documents/GitHub/text-Analysis/Pertemuan%203/Class_Meeting_3.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transformed_df\n",
            "\u001b[1;31mNameError\u001b[0m: name 'transformed_df' is not defined"
          ]
        }
      ],
      "source": [
        "transformed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVpzKEwW0UI4",
        "outputId": "a1d90fd9-7450-4410-abcc-89e9d364b478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity of the TF-IDF matrix: 0.9819\n"
          ]
        }
      ],
      "source": [
        "# Calculate the sparsity of the TF-IDF matrix\n",
        "tfidf_df =  transformed_df.drop(['at'\t,'content','score','userName','contentp','contentp_clean'], axis=1)\n",
        "sparsity = 1.0 - (np.count_nonzero(tfidf_df) / tfidf_df.size)\n",
        "\n",
        "print(f\"Sparsity of the TF-IDF matrix: {sparsity:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNxFAwWtQBsSWP8ajF7ueNO",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
