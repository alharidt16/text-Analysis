{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZtPKCvwYk96"
   },
   "source": [
    "# Week Class 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny7XH3kLYo6L"
   },
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igtVe066uOoV",
    "outputId": "eb1e115f-422d-4151-9a8a-d745eca24959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google_play_scraper in c:\\users\\alharidt\\appdata\\roaming\\python\\python311\\site-packages (1.2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install google_play_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H9B_POgIuWmK"
   },
   "outputs": [],
   "source": [
    "from google_play_scraper import Sort, reviews_all\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3KTn_46zuY9w"
   },
   "outputs": [],
   "source": [
    "result = reviews_all(\n",
    "        'com.myorbit',\n",
    "        sleep_milliseconds=0, # defaults to 0\n",
    "        lang='id', # defaults to 'en'\n",
    "        country='id', # defaults to 'us'\n",
    "        sort=Sort.NEWEST\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame.from_records(result)\n",
    "df = df[['at', 'content', 'score', 'userName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zzI4axF2ujum"
   },
   "outputs": [],
   "source": [
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "# Filter rows based on the condition: Timestamp >= '2023-01-01'\n",
    "filtered_df = df[df['at'] >= '2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "a8-hA2wFuxkJ",
    "outputId": "626bee32-1331-42ae-f46c-4701328f2144"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>userName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-12 11:08:59</td>\n",
       "      <td>Cukuplah, di tempat saya, speedtestnya selalu ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Effendy Lim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-18 21:58:35</td>\n",
       "      <td>Sering muncul tulisan \"tidak terhubung ke .......</td>\n",
       "      <td>3</td>\n",
       "      <td>Suryokusumo Risdika Rizki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-25 18:10:08</td>\n",
       "      <td>Masalah lancar ya lancar, namun kadang sering ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Sudirman Aja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-21 14:31:30</td>\n",
       "      <td>Sering muncul peringatan 'anda tidak terhubung...</td>\n",
       "      <td>5</td>\n",
       "      <td>Wilson Nts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-07 10:03:11</td>\n",
       "      <td>Masih banyak bug, kemaren2 beli paket ada diki...</td>\n",
       "      <td>1</td>\n",
       "      <td>Andi Arianto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19396</th>\n",
       "      <td>2023-09-29 00:08:34</td>\n",
       "      <td>👍</td>\n",
       "      <td>5</td>\n",
       "      <td>Muh Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19401</th>\n",
       "      <td>2023-01-18 22:03:06</td>\n",
       "      <td>🙏🙏🙏🙏</td>\n",
       "      <td>5</td>\n",
       "      <td>Meki Mote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19413</th>\n",
       "      <td>2023-04-02 18:06:26</td>\n",
       "      <td>👍👍👍👍</td>\n",
       "      <td>5</td>\n",
       "      <td>Bertus Soen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19423</th>\n",
       "      <td>2023-10-18 02:13:41</td>\n",
       "      <td>Terbaik👍👍👍👍👍</td>\n",
       "      <td>5</td>\n",
       "      <td>ANDYCCTV Pangkep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19429</th>\n",
       "      <td>2023-03-11 14:24:20</td>\n",
       "      <td>Oc..</td>\n",
       "      <td>5</td>\n",
       "      <td>Irpay Albantani</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3670 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       at                                            content  \\\n",
       "0     2023-11-12 11:08:59  Cukuplah, di tempat saya, speedtestnya selalu ...   \n",
       "1     2023-09-18 21:58:35  Sering muncul tulisan \"tidak terhubung ke .......   \n",
       "2     2023-10-25 18:10:08  Masalah lancar ya lancar, namun kadang sering ...   \n",
       "3     2023-09-21 14:31:30  Sering muncul peringatan 'anda tidak terhubung...   \n",
       "4     2023-11-07 10:03:11  Masih banyak bug, kemaren2 beli paket ada diki...   \n",
       "...                   ...                                                ...   \n",
       "19396 2023-09-29 00:08:34                                                  👍   \n",
       "19401 2023-01-18 22:03:06                                               🙏🙏🙏🙏   \n",
       "19413 2023-04-02 18:06:26                                               👍👍👍👍   \n",
       "19423 2023-10-18 02:13:41                                       Terbaik👍👍👍👍👍   \n",
       "19429 2023-03-11 14:24:20                                               Oc..   \n",
       "\n",
       "       score                   userName  \n",
       "0          4                Effendy Lim  \n",
       "1          3  Suryokusumo Risdika Rizki  \n",
       "2          4               Sudirman Aja  \n",
       "3          5                 Wilson Nts  \n",
       "4          1               Andi Arianto  \n",
       "...      ...                        ...  \n",
       "19396      5                     Muh Fc  \n",
       "19401      5                  Meki Mote  \n",
       "19413      5                Bertus Soen  \n",
       "19423      5           ANDYCCTV Pangkep  \n",
       "19429      5            Irpay Albantani  \n",
       "\n",
       "[3670 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1090703269.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -c \"import xml.etree.ElementTree\"\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"import xml.etree.ElementTree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I1tfJzvYtkw"
   },
   "source": [
    "## Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIl8FUsOvW6r",
    "outputId": "819bf8b3-c4dc-4aae-f9dc-dc3caf6f7241"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install sastrawi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUAam2kUY0X3"
   },
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtDSG5F2xKy4",
    "outputId": "5ee025ec-cc18-415e-eb42-adf3c03ad9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah korea selatan, jepang, singapura, hong kong, dan finlandia.\n"
     ]
    }
   ],
   "source": [
    "kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\n",
    "lower_case = kalimat.lower()\n",
    "print(lower_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjJPakERxUml",
    "outputId": "7ea12637-2cc3-4a78-c293-5fefefb5a0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berikut ini adalah  negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\n"
     ]
    }
   ],
   "source": [
    "import re # regular exoression modul\n",
    "kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\n",
    "hasil = re.sub(r\"\\d+\",'', kalimat)\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9JLfiXQzJuU",
    "outputId": "09d02e2c-d02d-42ab-8ce3-6125b5baca28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah contoh kalimat dengan tanda baca\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "kalimat = \"Ini &adalah [contoh] kalimat? {dengan} tanda. baca?!!\"\n",
    "hasil = kalimat.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "print(hasil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFKV-_CnzKdm",
    "outputId": "f5720e6b-ab81-4cf8-d121-585134b9ec6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t ini kalimat contoh\t \n",
      "ini kalimat contoh\n"
     ]
    }
   ],
   "source": [
    "kalimat = \" \\t ini kalimat contoh\\t \"\n",
    "hasil = kalimat.strip()\n",
    "print(kalimat)\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6TShX2_Ibnk",
    "outputId": "e29cc956-4e50-4c4b-b5cb-e58e162c1da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rumah', 'idaman', 'adalah', 'rumah', 'yang', 'bersih.']\n"
     ]
    }
   ],
   "source": [
    "kalimat = \"rumah idaman adalah rumah yang bersih.\"\n",
    "pisah = kalimat.split()\n",
    "print(pisah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34E3Ytu2Y8mF"
   },
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_IgEx3_0YYf",
    "outputId": "21f7fba8-16e6-470a-cbd5-4092b61d6029"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xml.etree'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#nltk.download('punkt')\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\__init__.py:103\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# \"Trove\" classifiers for Python Package Index.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m __classifiers__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevelopment Status :: 5 - Production/Stable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntended Audience :: Developers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic :: Text Processing :: Linguistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    101\u001b[0m ]\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_java\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# support numpy from pypy\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\internals.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElementTree\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Java Via Command-Line\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[0;32m     26\u001b[0m _java_bin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xml.etree'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "kalimat = \"rumah idaman adalah rumah yang bersih.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "tokens = nltk.tokenize.word_tokenize(kalimat)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NW0AULj1JYFj",
    "outputId": "cef1ac99-498f-45bf-8969-97e42a69679b"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(kalimat)\n",
    "kemunculan = nltk.FreqDist(tokens)\n",
    "print(kemunculan.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "rfuGJuiqJx26",
    "outputId": "68252634-1691-44ad-abae-9bca2ac340f1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "kemunculan.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJ5BGPLbKUui",
    "outputId": "0bf901e8-0314-4186-d7af-a7d128cc0f0c"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "\n",
    "tokens = nltk.tokenize.sent_tokenize(kalimat)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GroA-w_ZZF8m"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-iQWA0lLGN2",
    "outputId": "ca3155df-f364-43a2-fd49-18f79cad5f89"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "tokens = word_tokenize(kalimat)\n",
    "listStopword =  set(stopwords.words('indonesian'))\n",
    "\n",
    "removed = []\n",
    "for t in tokens:\n",
    "    if t not in listStopword:\n",
    "        removed.append(t)\n",
    "\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qc6AQ9eMZc_",
    "outputId": "0afdc7b1-601b-4309-c41e-8d0d066b4789"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "stop = stopword.remove(kalimat)\n",
    "tokens = nltk.tokenize.word_tokenize(stop)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whB4qr5BM3iZ",
    "outputId": "7d22c90e-b1c8-4507-e827-d6121e3a7f2f"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Create a factory for the stopword remover\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "\n",
    "# Define your custom stopwords\n",
    "custom_stopwords = [\"andi\"]\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "\n",
    "# Remove punctuation and convert to lowercase\n",
    "kalimat = kalimat.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "# Apply Sastrawi stopword removal\n",
    "stop = stopword.remove(kalimat)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.tokenize.word_tokenize(stop)\n",
    "\n",
    "# Remove custom stopwords\n",
    "tokens = [x for x in tokens if x not in custom_stopwords]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYBL5PIVZKKM"
   },
   "source": [
    "###Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lD7RjLAjNTRs",
    "outputId": "54a644cc-fea4-4902-ba05-d5014afea2b4"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "kata = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n",
    "\n",
    "for k in kata:\n",
    "    print(k, \" : \", ps.stem(k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0aZiLldQE5z",
    "outputId": "aa2ca9e5-8262-462f-8286-bfbae3cd04d0"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "hasil = stemmer.stem(kalimat)\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "LY4n33x1QggP",
    "outputId": "f159efea-5740-4630-ad0a-0239b97889e5"
   },
   "outputs": [],
   "source": [
    "filtered_df2 = filtered_df\n",
    "filtered_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoEmjk53YVry"
   },
   "source": [
    "### Case with python with orbit data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHLhBgTkUoN5",
    "outputId": "e01c27dc-72ea-47e4-af97-b98caffe39f8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def stemmerSastrawi(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "def stopwordSastrawi(text):\n",
    "    return stopword.remove(text)\n",
    "\n",
    "def regex(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^.,a-zA-Z0-9 \\n\\.]',' ',text) #remove symbol\n",
    "    text = re.sub('[\\s]+', ' ', text) #menghilangkan additional whitespace\n",
    "    text = re.sub(r'[^\\w\\s]','',text) #remove punctuation\n",
    "    text = text.strip() #menghilangkan enter, tab, dll\n",
    "    return text\n",
    "\n",
    "def ka_handling(text):\n",
    "    tokens = text.split()\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token in ka.keys():\n",
    "            tokens[i] = ka[token]\n",
    "    return ''.join(' '.join(x for x in tokens))\n",
    "\n",
    "def addt_stop_word(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return ''.join(' '.join(w for w in words if not w.lower() in stop_words))\n",
    "\n",
    "ka = pd.read_csv('kamus_alay.csv', encoding = \"ISO-8859-1\", header=None)\n",
    "ka.columns = ['matab','mantap']\n",
    "ka = {ka.matab[i]:ka.mantap[i] for i in range(0,len(ka.matab.values))}\n",
    "\n",
    "stop_words = {'dan', 'di', 'ini',  'USER', 'User'}\n",
    "\n",
    "\n",
    "def all_preproc(input_data):\n",
    "    preproc = list(map(regex, input_data.values))\n",
    "    preproc = list(map(ka_handling, preproc))\n",
    "    preproc = list(map(stemmerSastrawi, tqdm(preproc)))\n",
    "    preproc = list(map(stopwordSastrawi, tqdm(preproc)))\n",
    "    preproc = list(map(addt_stop_word, tqdm(preproc)))\n",
    "    return preproc\n",
    "\n",
    "filtered_df2['contentp'] = filtered_df2['content'].astype(str).apply(lambda x: ' '.join(simple_preprocess(x)))\n",
    "filtered_df2['contentp_clean'] = all_preproc(filtered_df2['contentp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "_orhnb1gWOBH",
    "outputId": "f5ec6187-bda6-48a5-f483-d999c2492c82"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "filtered_df2['text_length'] = filtered_df2['contentp_clean'].apply(len)\n",
    "\n",
    "# Plot the distribution of text length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=filtered_df2, x='text_length', bins=50, kde=True)\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xdlf-YonXjXy",
    "outputId": "be00e544-6ad8-437b-93dc-c88814e79054"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize and count word occurrences\n",
    "word_counts = Counter(word_tokenize(' '.join(filtered_df2['contentp_clean'])))\n",
    "most_common_words = word_counts.most_common(30)  # Get the top 20 most common words\n",
    "\n",
    "# Plot a bar chart for the most common words\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=[word[0] for word in most_common_words], y=[word[1] for word in most_common_words])\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Create a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "b75TK6IgXsd0",
    "outputId": "b45629be-2446-43a0-82a6-a96f827fcc5c"
   },
   "outputs": [],
   "source": [
    "filtered_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2.to_csv('clean_review_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHzjDiIboxSZRt3dGv9ho+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
